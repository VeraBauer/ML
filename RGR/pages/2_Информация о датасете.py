import pandas as pd
import streamlit as st

st.set_page_config(layout="wide")

st.title("Информация о датасете")

st.sidebar.header("Информация о датасете")

data = pd.read_csv('..\data\smoke_detector.csv')

data.drop(data.columns[[0]], axis=1, inplace=True)

col1, col2 = st.columns(2)

col1.dataframe(data)

col2.image('../ionization-smoke-detectors.jpg', width=400)

st.markdown(
    """
    Датасет :blue[«Smoke Detection Dataset»] - набор данных для бинарной классификации, который используется для определения возможного возгорания. Изначально сбор данных осуществлялся для разработки детектора дыма на основе искуственного интеллекта. Датасет содержит более 60 000 подробных сведений, влияющих на наличие пожара, таких как:
    """
)

st.markdown(
    """
    * Температура воздуха в градусах Цельсия (-22 - 59.9)

    * Влажность воздуха в процентах (10.7 - 75.2)

    * Общее количество летучих органических соединений в частицах на миллиард (0 - 60 000)

    * Эквивалентная концентрация CO2 в частицах на миллион (400 - 60 000)
    
    * Содержание неочищенного молекулярного водорода в частицах на миллион (10 700 - 13 800)

    * Содержание газообразного этанола в частицах на миллион (15 300 - 21 400)

    * Давление воздуха в гПа (931 - 940)
    
    * Число твердых частиц димаметром менее 1 микрона и диаметром более 1 микрона, но менее 2,5 микрон на кубометр (0 - 45 400)
    
    * Численная концентрация твердых частиц в воздухе (0 - 61 500)

    * Пожарная тревога ('0'— нет пожара, '1'— обнаружено возгорание) — предсказываемый признак, оповещающий о возможном возгорании
    """
)
st.markdown(
    """
    В процессе предобработки датасета были заполнены пустые значения, которых в общей сложности по всем столбцам было более 2 000, все данные приведены к типу float. Дубликатов, к счастью, не оказалось, поэтому удалять их не пришлось. Также, данные были сбалансированны с помощью метода RandomUnderSampler библиотеки imblearn и масштабированы с помощью MinMaxScaler библиотеки sklearn.
    """
)